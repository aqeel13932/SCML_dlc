{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense,Input,Lambda,regularizers,BatchNormalization,initializers,Activation\n",
    "from keras.optimizers import adam\n",
    "from keras.models import Model,load_model\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import sys\n",
    "import pandas as pd\n",
    "from task import Task\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base(ABC):\n",
    "    def __init__(self,in_size,out_size,act_min,act_max,lr=0.001,nl=2,activation=['relu','relu','sigmoid'],num_nodes\n",
    "            =[400,300],tau=0.001,kr=None):\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.act_min = act_min\n",
    "        self.act_max = act_max\n",
    "        self.nl=nl\n",
    "        self.lr = lr\n",
    "        self.num_nodes = num_nodes\n",
    "        self.activations = activation\n",
    "        self.kernel_regularizer = kr\n",
    "        self.Create_network()\n",
    "    #@abstractmethod\n",
    "    def Create_network(self):\n",
    "        pass\n",
    "    #@abstractmethod\n",
    "    def train(self):\n",
    "        pass\n",
    "    def Update(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "?Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-23-e0c516e377bd>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-e0c516e377bd>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    class Critic(Base):\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class Actor(Base):\n",
    "    def Create_network(self):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "\n",
    "        net = layers.Dense(units=400, kernel_regularizer=layers.regularizers.l2(1e-6))(states)\n",
    "        \n",
    "        net = layers.Dense(units=300, kernel_regularizer=layers.regularizers.l2(1e-6))(net)\n",
    "\n",
    "        # Add final output layer with weights initialised to Uniform[-3e-3, 3e-3]\n",
    "        #raw_actions = layers.Dense(units=self.action_size, activation='sigmoid', name='raw_actions')(net)\n",
    "        raw_actions = layers.Dense(units=self.action_size, activation='sigmoid', kernel_initializer = layers.initializers.RandomUniform(minval=-0.003, maxval=0.003), name='raw_actions')(net)\n",
    "\n",
    "        # Scale [0, 1] output for each action dimension to proper range\n",
    "        actions = layers.Lambda(lambda x: (x * self.action_range) + self.action_low, name='actions')(raw_actions)\n",
    "\n",
    "        # Create Keras model\n",
    "        self.model = models.Model(inputs=states, outputs=actions)\n",
    "\n",
    "        # Define loss function using action value (Q value) gradients\n",
    "        action_gradients = layers.Input(shape=(self.action_size,))\n",
    "        loss = K.mean(-action_gradients * actions)\n",
    "\n",
    "        # Incorporate any additional losses here (e.g. from regularizers)\n",
    "\n",
    "        # Define optimizer and training function\n",
    "        optimizer = optimizers.Adam(lr=self.learning_rate)\n",
    "        updates_op = optimizer.get_updates(params=self.model.trainable_weights, loss=loss)\n",
    "        self.train_fn = K.function(\n",
    "            inputs=[self.model.input, action_gradients, K.learning_phase()],\n",
    "            outputs=[],\n",
    "            updates=updates_op)\n",
    "        \n",
    "        \n",
    "        x = Input((self.in_size,))\n",
    "        out=None\n",
    "        for i in range(self.nl):\n",
    "            if out==None:\n",
    "                out=Dense(self.num_nodes[i])(x)\n",
    "            else:\n",
    "                out=Dense(self.num_nodes[i])(out)\n",
    "            if(self.BN):\n",
    "                out=BatchNormalization()(out)\n",
    "            out = Activation(self.activations[i])\n",
    "        return Model(inputs=[inp],outputs=[out]),inp\n",
    "\n",
    "class Critic(Base):\n",
    "    def __init__(self,in_size,out_size,act_min,act_max,lr=0.001,nl=2,activation=['relu','relu','sigmoid'],num_nodes=[400,300],tau=0.001,actin_size):\n",
    "        super().__init__(in_size,out_size,act_min,act_max,lr,nl,activation,num_nodes,tau)\n",
    "        self.actin_size= actin_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blabla\n",
      "[400, 300]\n"
     ]
    }
   ],
   "source": [
    "ss = Actor(18,4,0,900)\n",
    "ss = Critic(18,4,1,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class Actor with abstract methods train",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-db44f85fc118>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class Actor with abstract methods train"
     ]
    }
   ],
   "source": [
    "x = Actor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "class Actor:\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, action_low, action_high):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            action_low (array): Min value of each action dimension\n",
    "            action_high (array): Max value of each action dimension\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.action_low = action_low\n",
    "        self.action_high = action_high\n",
    "        self.action_range = self.action_high - self.action_low\n",
    "\n",
    "        # Initialize any other variables here\n",
    "        self.learning_rate = 0.0001\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "class Critic:\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Initialize any other variables here\n",
    "        self.learning_rate = 0.001\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        # Define input layers\n",
    "        states = layers.Input(shape=(self.state_size,), name='states')\n",
    "        actions = layers.Input(shape=(self.action_size,), name='actions')\n",
    "\n",
    "        # Add hidden layer(s) for state pathway\n",
    "        #net_states = layers.Dense(units=32, activation='relu')(states)\n",
    "        #net_states = layers.Dense(units=64, activation='relu')(net_states)\n",
    "\n",
    "        net_states = layers.Dense(units=400, kernel_regularizer=layers.regularizers.l2(1e-6))(states)\n",
    "        net_states = layers.BatchNormalization()(net_states)\n",
    "        net_states = layers.Activation('relu')(net_states)\n",
    "\n",
    "        # Add hidden layer(s) for action pathway\n",
    "        net_states = layers.Dense(units=300, kernel_regularizer=layers.regularizers.l2(1e-6))(net_states)\n",
    "        net_actions = layers.Dense(units=300, kernel_regularizer=layers.regularizers.l2(1e-6))(actions)\n",
    "\n",
    "        # Try different layer sizes, activations, add batch normalization, regularizers, etc.\n",
    "\n",
    "        # Combine state and action pathways\n",
    "        net = layers.Add()([net_states, net_actions])\n",
    "        net = layers.Activation('relu')(net)\n",
    "\n",
    "        # Add more layers to the combined network if needed\n",
    "\n",
    "        # Add final output layer to produce action values (Q values)\n",
    "        #Q_values = layers.Dense(units=1, name='q_values')(net)\n",
    "        Q_values = layers.Dense(units=1, name='q_values', kernel_initializer = layers.initializers.RandomUniform(minval=-0.003, maxval=0.003))(net)\n",
    "\n",
    "        # Create Keras model\n",
    "        self.model = models.Model(inputs=[states, actions], outputs=Q_values)\n",
    "\n",
    "        # Define optimizer and compile model for training with built-in loss function\n",
    "        optimizer = optimizers.Adam(lr=self.learning_rate)\n",
    "        self.model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        # Compute action gradients (derivative of Q values w.r.t. to actions)\n",
    "        action_gradients = K.gradients(Q_values, actions)\n",
    "\n",
    "        # Define an additional function to fetch action gradients (to be used by actor model)\n",
    "        self.get_action_gradients = K.function(\n",
    "            inputs=[*self.model.input, K.learning_phase()],\n",
    "            outputs=action_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG_Agent():\n",
    "    \"\"\"Reinforcement Learning agent that learns using DDPG.\"\"\"\n",
    "    def __init__(self, task):\n",
    "        self.task = task\n",
    "        self.state_size = task.state_size\n",
    "        self.action_size = task.action_size\n",
    "        self.action_low = task.action_low\n",
    "        self.action_high = task.action_high\n",
    "\n",
    "        # Actor (Policy) Model\n",
    "        self.actor_local = Actor(self.state_size, self.action_size, self.action_low, self.action_high)\n",
    "        self.actor_target = Actor(self.state_size, self.action_size, self.action_low, self.action_high)\n",
    "\n",
    "        # Critic (Value) Model\n",
    "        self.critic_local = Critic(self.state_size, self.action_size)\n",
    "        self.critic_target = Critic(self.state_size, self.action_size)\n",
    "\n",
    "        # Initialize target model parameters with local model parameters\n",
    "        self.critic_target.model.set_weights(self.critic_local.model.get_weights())\n",
    "        self.actor_target.model.set_weights(self.actor_local.model.get_weights())\n",
    "\n",
    "        # Noise process long-running mean / the speed of mean reversion / the volatility parameter\n",
    "        self.exploration_mu = 0\n",
    "        self.exploration_theta = 0.15\n",
    "        #self.exploration_sigma = 0.3\n",
    "        self.exploration_sigma = 0.2\n",
    "        self.noise = OUNoise(self.action_size, self.exploration_mu, self.exploration_theta, self.exploration_sigma)\n",
    "\n",
    "        # Replay memory\n",
    "        self.buffer_size = 1000000\n",
    "        self.batch_size = 64\n",
    "        self.memory = ReplayBuffer(self.buffer_size, self.batch_size)\n",
    "\n",
    "        # Algorithm parameters\n",
    "        self.gamma = 0.99  # discount factor\n",
    "        self.tau = 0.001  # for soft update of target parameters\n",
    " \n",
    "    def reset_episode(self):\n",
    "        self.noise.reset()\n",
    "        state = self.task.reset()\n",
    "        self.last_state = state\n",
    "        return state\n",
    "\n",
    "    def step(self, action, reward, next_state, done):\n",
    "         # Save experience / reward\n",
    "        self.memory.add(self.last_state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences)\n",
    "\n",
    "        # Roll over last state and action\n",
    "        self.last_state = next_state\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Returns actions for given state(s) as per current policy.\"\"\"\n",
    "        state = np.reshape(state, [-1, self.state_size])\n",
    "        action = self.actor_local.model.predict(state)[0]\n",
    "        return list(action + self.noise.sample())  # add some noise for exploration\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\"\"\"\n",
    "        # Convert experience tuples to separate arrays for each element (states, actions, rewards, etc.)\n",
    "        states = np.vstack([e.state for e in experiences if e is not None])\n",
    "        actions = np.array([e.action for e in experiences if e is not None]).astype(np.float32).reshape(-1, self.action_size)\n",
    "        rewards = np.array([e.reward for e in experiences if e is not None]).astype(np.float32).reshape(-1, 1)\n",
    "        dones = np.array([e.done for e in experiences if e is not None]).astype(np.uint8).reshape(-1, 1)\n",
    "        next_states = np.vstack([e.next_state for e in experiences if e is not None])\n",
    "\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        #     Q_targets_next = critic_target(next_state, actor_target(next_state))\n",
    "        actions_next = self.actor_target.model.predict_on_batch(next_states)\n",
    "        Q_targets_next = self.critic_target.model.predict_on_batch([next_states, actions_next])\n",
    "\n",
    "        # Compute Q targets for current states and train critic model (local)\n",
    "        Q_targets = rewards + self.gamma * Q_targets_next * (1 - dones)\n",
    "        self.critic_local.model.train_on_batch(x=[states, actions], y=Q_targets)\n",
    "\n",
    "        # Train actor model (local)\n",
    "        action_gradients = np.reshape(self.critic_local.get_action_gradients([states, actions, 0]), (-1, self.action_size))\n",
    "        self.actor_local.train_fn([states, action_gradients, 1])  # custom training function\n",
    "\n",
    "        # Soft-update target models\n",
    "        self.soft_update(self.critic_local.model, self.critic_target.model)\n",
    "        self.soft_update(self.actor_local.model, self.actor_target.model)   \n",
    "\n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters.\"\"\"\n",
    "        local_weights = np.array(local_model.get_weights())\n",
    "        target_weights = np.array(target_model.get_weights())\n",
    "\n",
    "        assert len(local_weights) == len(target_weights), \"Local and target model parameters must have the same size\"\n",
    "\n",
    "        new_weights = self.tau * local_weights + (1 - self.tau) * target_weights\n",
    "        target_model.set_weights(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode =  381, score =  74.538, low score =   6.820, high score = 579.943"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-45d42e4eb8b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-6b66f4993ed7>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Roll over last state and action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-6b66f4993ed7>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m#     Q_targets_next = critic_target(next_state, actor_target(next_state))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mactions_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mQ_targets_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_next\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# Compute Q targets for current states and train critic model (local)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/SCML/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1272\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1274\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/SCML/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/SCML/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/SCML/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## TODO: Train your agent here\n",
    "import os\n",
    "import pickle\n",
    "#from tasks.task import Task\n",
    "\n",
    "exportPath = './data/'\n",
    "if not os.path.exists(exportPath):\n",
    "    os.makedirs(exportPath)\n",
    "\n",
    "# z axis is up\n",
    "init_pose = np.array([0.0, 0.0, 1.0, 0.0, 0.0, 0.0]) \n",
    "target_pos = np.array([0.0, 0.0, 10.0])\n",
    "task = Task(init_pose=init_pose, target_pos=target_pos, runtime=5.0)\n",
    "#agent = PolicySearch_Agent(task) \n",
    "#agent = Basic_Agent(task) \n",
    "agent = DDPG_Agent(task)\n",
    "\n",
    "# before training\n",
    "resultsAll = []\n",
    "high_score = -1000000.0\n",
    "low_score = 1000000.0\n",
    "\n",
    "# number of episodes to train\n",
    "num_episodes = 750\n",
    "\n",
    "training_results = {\n",
    "    'score': [],\n",
    "    }\n",
    "\n",
    "for i_episode in range(1, num_episodes+1):\n",
    "    # start a new episode\n",
    "    state = agent.reset_episode() \n",
    "    score = 0\n",
    "\n",
    "    episode_results = {\n",
    "        'time': [],\n",
    "        'x': [],\n",
    "        'y': [],\n",
    "        'z': [],\n",
    "        'phi': [],\n",
    "        'theta': [],\n",
    "        'psi': [],\n",
    "        'vx': [],\n",
    "        'vy': [],\n",
    "        'vz': [],\n",
    "        'reward': [],\n",
    "        }\n",
    "\n",
    "    while True:\n",
    "        action = agent.act(state)  \n",
    "        next_state, reward, done = task.step(action)\n",
    "\n",
    "        agent.step(action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        high_score = max(high_score, score)\n",
    "        low_score = min(low_score, score)\n",
    "\n",
    "        # track the results for offline analysis\n",
    "        episode_results['time'].append(task.sim.time)\n",
    "        episode_results['x'].append(state[0])\n",
    "        episode_results['y'].append(state[1])\n",
    "        episode_results['z'].append(state[2])\n",
    "        episode_results['phi'].append(state[3])\n",
    "        episode_results['theta'].append(state[4])\n",
    "        episode_results['psi'].append(state[5])\n",
    "        episode_results['vx'].append(state[6])\n",
    "        episode_results['vy'].append(state[7])\n",
    "        episode_results['vz'].append(state[8])\n",
    "        episode_results['reward'].append(reward)\n",
    "        \n",
    "        if done:\n",
    "            print(\"\\rEpisode = {:4d}, score = {:7.3f}, low score = {:7.3f}, high score = {:7.3f}\".format(i_episode, score, low_score, high_score), end=\"\")\n",
    "            training_results['score'].append(score)\n",
    "            break\n",
    "\n",
    "    resultsAll.append(episode_results)\n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# save results for later analysis\n",
    "with open(\"{}results0.bin\".format(exportPath), 'wb') as pickleFile:\n",
    "    pickle.dump(resultsAll, pickleFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode =  500, reward =  84.504 , steps count = 20"
     ]
    }
   ],
   "source": [
    "ch =[]\n",
    "ah =[]\n",
    "for i_episode in range(1, num_episodes+1):\n",
    "    state = agent.Reset() # start a new episode\n",
    "    episode_reward=0\n",
    "    steps_count =0\n",
    "    while True:\n",
    "        action = agent.Get_action(state[np.newaxis,:])\n",
    "        next_state, reward, done = task.step(action[0]*900)\n",
    "        steps_count+=1\n",
    "        episode_reward+=reward\n",
    "        agent.Replay.add(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        info = agent.train_models()\n",
    "        if info:\n",
    "            ch.append(info[0])\n",
    "            ah.append(info[1])\n",
    "        if done:\n",
    "            print(\"\\rEpisode = {:4d}, reward = {:7.3f} , steps count = {}\".format(\n",
    "                i_episode, episode_reward,steps_count), end=\"\")  # [debug]\n",
    "            reward_results['episode'].append(i_episode)\n",
    "            reward_results['reward'].append(episode_reward)\n",
    "            break\n",
    "    sys.stdout.flush()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-239bc5d53fcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# z axis is up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0minit_pose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtarget_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_pose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_pose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mruntime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "#from tasks.task import Task\n",
    "\n",
    "exportPath = './data/'\n",
    "if not os.path.exists(exportPath):\n",
    "    os.makedirs(exportPath)\n",
    "\n",
    "# z axis is up\n",
    "init_pose = np.array([0.0, 0.0, 1.0, 0.0, 0.0, 0.0]) \n",
    "target_pos = np.array([0.0, 0.0, 10.0])\n",
    "task = Task(init_pose=init_pose, target_pos=target_pos, runtime=5.0)\n",
    "#agent = PolicySearch_Agent(task) \n",
    "#agent = Basic_Agent(task) \n",
    "agent = DDPG_Agent(task)\n",
    "\n",
    "# before training\n",
    "resultsAll = []\n",
    "high_score = -1000000.0\n",
    "low_score = 1000000.0\n",
    "\n",
    "# number of episodes to train\n",
    "num_episodes = 750\n",
    "\n",
    "training_results = {\n",
    "    'score': [],\n",
    "    }\n",
    "\n",
    "for i_episode in range(1, num_episodes+1):\n",
    "    # start a new episode\n",
    "    state = agent.reset_episode() \n",
    "    score = 0\n",
    "\n",
    "    episode_results = {\n",
    "        'time': [],\n",
    "        'x': [],\n",
    "        'y': [],\n",
    "        'z': [],\n",
    "        'phi': [],\n",
    "        'theta': [],\n",
    "        'psi': [],\n",
    "        'vx': [],\n",
    "        'vy': [],\n",
    "        'vz': [],\n",
    "        'reward': [],\n",
    "        }\n",
    "\n",
    "    while True:\n",
    "        action = agent.act(state)  \n",
    "        next_state, reward, done = task.step(action)\n",
    "\n",
    "        agent.step(action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        high_score = max(high_score, score)\n",
    "        low_score = min(low_score, score)\n",
    "\n",
    "        # track the results for offline analysis\n",
    "        episode_results['time'].append(task.sim.time)\n",
    "        episode_results['x'].append(state[0])\n",
    "        episode_results['y'].append(state[1])\n",
    "        episode_results['z'].append(state[2])\n",
    "        episode_results['phi'].append(state[3])\n",
    "        episode_results['theta'].append(state[4])\n",
    "        episode_results['psi'].append(state[5])\n",
    "        episode_results['vx'].append(state[6])\n",
    "        episode_results['vy'].append(state[7])\n",
    "        episode_results['vz'].append(state[8])\n",
    "        episode_results['reward'].append(reward)\n",
    "        \n",
    "        if done:\n",
    "            print(\"\\rEpisode = {:4d}, score = {:7.3f}, low score = {:7.3f}, high score = {:7.3f}\".format(i_episode, score, low_score, high_score), end=\"\")\n",
    "            training_results['score'].append(score)\n",
    "            break\n",
    "\n",
    "    resultsAll.append(episode_results)\n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# save results for later analysis\n",
    "with open(\"{}results0.bin\".format(exportPath), 'wb') as pickleFile:\n",
    "    pickle.dump(resultsAll, pickleFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
